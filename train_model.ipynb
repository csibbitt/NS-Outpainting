{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb73211-61bd-462c-9222-c0ab5024e931",
   "metadata": {},
   "outputs": [],
   "source": [
    "! if [ ! -f tf_scenery.zip ]; then wget \"https://drive.google.com/uc?export=download&id=1XcL0guFyqhLns_HgkFBEKEpbUHQ1dA6U&confirm=yes\" -O tf_scenery.zip; unzip tf_scenery.zip; fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0ecc01-776d-423f-b305-90df9e358b0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow-addons\n",
    "import random\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('./content/')\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.training.moving_averages import assign_moving_average\n",
    "from modeling.model import Model\n",
    "from modeling.loss import Loss\n",
    "from dataset.parse import parse_trainset, parse_testset\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db68f1be-56b2-485c-a5af-3fc0d8065299",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='Model training.')\n",
    "# experiment\n",
    "parser.add_argument('--date', type=str, default='1214')\n",
    "parser.add_argument('--exp-index', type=int, default=3)\n",
    "parser.add_argument('--f', action='store_true', default=False)\n",
    "\n",
    "# gpu\n",
    "parser.add_argument('--start-gpu', type=int, default=0)\n",
    "parser.add_argument('--num-gpu', type=int, default=2)\n",
    "\n",
    "# dataset\n",
    "parser.add_argument('--trainset-path', type=str, default='./dataset/trainset.tfr')\n",
    "parser.add_argument('--testset-path', type=str, default='./dataset/testset.tfr')\n",
    "parser.add_argument('--trainset-length', type=int, default=5041)\n",
    "parser.add_argument('--testset-length', type=int, default=2000)  # we flip every image in testset\n",
    "\n",
    "# training\n",
    "parser.add_argument('--base-lr', type=float, default=0.0001)\n",
    "parser.add_argument('--batch-size', type=int, default=32)\n",
    "parser.add_argument('--weight-decay', type=float, default=0.00002)\n",
    "parser.add_argument('--epoch', type=int, default=1500)\n",
    "parser.add_argument('--lr-decay-epoch', type=int, default=1000)\n",
    "parser.add_argument('--critic-steps', type=int, default=3)\n",
    "parser.add_argument('--warmup-steps', type=int, default=1000)\n",
    "parser.add_argument('--workers', type=int, default=2)\n",
    "parser.add_argument('--clip-gradient', action='store_true', default=False)\n",
    "parser.add_argument('--clip-gradient-value', type=float, default=0.1)\n",
    "\n",
    "\n",
    "# modeling\n",
    "parser.add_argument('--beta', type=float, default=0.9)\n",
    "parser.add_argument('--lambda-gp', type=float, default=10)\n",
    "parser.add_argument('--lambda-rec', type=float, default=0.998)\n",
    "\n",
    "# checkpoint\n",
    "parser.add_argument('--log-path', type=str, default='./logs/')\n",
    "parser.add_argument('--checkpoint-path', type=str, default=None)\n",
    "parser.add_argument('--resume-step', type=int, default=0)\n",
    "\n",
    "args = parser.parse_args(['--trainset-path', './dataset/trainset.tfr', '--testset-path', './dataset/testset.tfr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7bf839-96fb-4050-87c6-52b0e34b54c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# prepare path\n",
    "base_path = args.log_path\n",
    "exp_date = args.date\n",
    "if exp_date is None:\n",
    "    print('Exp date error!')\n",
    "    import sys\n",
    "    sys.exit()\n",
    "exp_name = exp_date + '/' + str(args.exp_index)\n",
    "print(\"Start Exp:\", exp_name)\n",
    "output_path = base_path + exp_name + '/'\n",
    "model_path = output_path + 'models/'\n",
    "tensorboard_path = output_path + 'log/'\n",
    "result_path = output_path + 'results/'\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "if not os.path.exists(tensorboard_path):\n",
    "    os.makedirs(tensorboard_path)\n",
    "if not os.path.exists(result_path):\n",
    "    os.makedirs(result_path)\n",
    "elif not args.f:\n",
    "    if args.checkpoint_path is None:\n",
    "        print('Exp exist!')\n",
    "        import sys\n",
    "        sys.exit()\n",
    "else:\n",
    "    import shutil\n",
    "    shutil.rmtree(model_path)\n",
    "    os.makedirs(model_path)\n",
    "    shutil.rmtree(tensorboard_path)\n",
    "    os.makedirs(tensorboard_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c761d337-c924-4396-b3e8-fca323054d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare gpu\n",
    "num_gpu = args.num_gpu\n",
    "start_gpu = args.start_gpu\n",
    "gpu_id = str(start_gpu)\n",
    "for i in range(num_gpu - 1):\n",
    "    gpu_id = gpu_id + ',' + str(start_gpu + i + 1)\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(gpu_id)\n",
    "args.batch_size_per_gpu = int(args.batch_size / args.num_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12515d6-32c6-4611-8da1-3e8922f8b2f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Model(args)\n",
    "loss = Loss(args)\n",
    "\n",
    "config = tf.compat.v1.ConfigProto(allow_soft_placement=True)\n",
    "config.gpu_options.allow_growth = True\n",
    "config.graph_options.optimizer_options.global_jit_level = tf.compat.v1.OptimizerOptions.ON_1\n",
    "\n",
    "print(\"Start building model...\")\n",
    "with tf.compat.v1.Session(config=config) as sess:\n",
    "    with tf.device('/cpu:0'):\n",
    "        learning_rate = tf.compat.v1.placeholder(tf.float32, [])\n",
    "        lambda_rec = tf.compat.v1.placeholder(tf.float32, [])\n",
    "\n",
    "        train_op_G = tf.compat.v1.train.AdamOptimizer(\n",
    "            learning_rate=learning_rate, beta1=0.5, beta2=0.9)\n",
    "        train_op_D = tf.compat.v1.train.AdamOptimizer(\n",
    "            learning_rate=learning_rate, beta1=0.5, beta2=0.9)\n",
    "\n",
    "\n",
    "        trainset = tf.compat.v1.data.TFRecordDataset(filenames=[args.trainset_path])\n",
    "        trainset = trainset.shuffle(args.trainset_length)\n",
    "        trainset = trainset.map(parse_trainset, num_parallel_calls=args.workers)\n",
    "        trainset = trainset.batch(args.batch_size).repeat()\n",
    "\n",
    "        train_iterator = trainset.make_one_shot_iterator()\n",
    "        train_im = train_iterator.get_next()\n",
    "\n",
    "        testset =tf.compat.v1.data.TFRecordDataset(filenames=[args.testset_path])\n",
    "        testset = testset.map(parse_testset, num_parallel_calls=args.workers)\n",
    "        testset = testset.batch(args.batch_size).repeat()\n",
    "\n",
    "        test_iterator = testset.make_one_shot_iterator()\n",
    "        test_im = test_iterator.get_next()\n",
    "\n",
    "        print('build model on gpu tower')\n",
    "        models = []\n",
    "        params = []\n",
    "        for gpu_id in range(num_gpu):\n",
    "            with tf.device('/gpu:%d' % gpu_id):\n",
    "                print('tower_%d' % gpu_id)\n",
    "                with tf.name_scope('tower_%d' % gpu_id):\n",
    "                    with tf.compat.v1.variable_scope('cpu_variables', reuse=gpu_id > 0):\n",
    "\n",
    "                        groundtruth = tf.compat.v1.placeholder(\n",
    "                            tf.float32, [args.batch_size_per_gpu, 128, 256, 3], name='groundtruth')\n",
    "                        left_gt = tf.slice(groundtruth, [0, 0, 0, 0], [args.batch_size_per_gpu, 128, 128, 3])\n",
    "\n",
    "\n",
    "                        reconstruction_ori, reconstruction = model.build_reconstruction(left_gt)\n",
    "                        right_recon = tf.slice(reconstruction, [0, 0, 128, 0], [args.batch_size_per_gpu, 128, 128, 3])\n",
    "\n",
    "                        loss_rec = loss.masked_reconstruction_loss(groundtruth, reconstruction)\n",
    "                        loss_adv_G, loss_adv_D = loss.global_and_local_adv_loss(model, groundtruth, reconstruction)\n",
    "\n",
    "                        reg_losses = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.REGULARIZATION_LOSSES)\n",
    "                        loss_G = loss_adv_G * (1 - lambda_rec) + loss_rec * lambda_rec + sum(reg_losses)\n",
    "                        loss_D = loss_adv_D\n",
    "\n",
    "                        var_G = list(filter(lambda x: x.name.startswith(\n",
    "                            'cpu_variables/GEN'), tf.compat.v1.trainable_variables()))\n",
    "                        var_D = list(filter(lambda x: x.name.startswith(\n",
    "                            'cpu_variables/DIS'), tf.compat.v1.trainable_variables()))\n",
    "\n",
    "\n",
    "                        grad_g = train_op_G.compute_gradients(\n",
    "                            loss_G, var_list=var_G)\n",
    "                        grad_d = train_op_D.compute_gradients(\n",
    "                            loss_D, var_list=var_D)\n",
    "\n",
    "                        models.append((grad_g, grad_d, loss_G, loss_D, loss_adv_G, loss_rec, reconstruction))\n",
    "                        params.append(groundtruth)\n",
    "\n",
    "        print('Done.')\n",
    "\n",
    "        print('Start reducing towers on cpu...')\n",
    "\n",
    "        grad_gs, grad_ds, loss_Gs, loss_Ds, loss_adv_Gs, loss_recs, reconstructions = zip(*models)\n",
    "        groundtruths = params\n",
    "        \n",
    "        with tf.device('/gpu:0'):\n",
    "            aver_loss_g = tf.reduce_mean(input_tensor=loss_Gs)\n",
    "            aver_loss_d = tf.reduce_mean(input_tensor=loss_Ds)\n",
    "            aver_loss_ag = tf.reduce_mean(input_tensor=loss_adv_Gs)\n",
    "            aver_loss_rec = tf.reduce_mean(input_tensor=loss_recs)\n",
    "\n",
    "            train_op_G = train_op_G.apply_gradients(\n",
    "                loss.average_gradients(grad_gs))\n",
    "            train_op_D = train_op_D.apply_gradients(\n",
    "                loss.average_gradients(grad_ds))\n",
    "\n",
    "            groundtruths = tf.concat(groundtruths, axis=0)\n",
    "            reconstructions = tf.concat(reconstructions, axis=0)\n",
    "\n",
    "            tf.compat.v1.summary.scalar('loss_g', aver_loss_g)\n",
    "            tf.compat.v1.summary.scalar('loss_d', aver_loss_d)\n",
    "            tf.compat.v1.summary.scalar('loss_ag', aver_loss_ag)\n",
    "            tf.compat.v1.summary.scalar('loss_rec', aver_loss_rec)\n",
    "            tf.compat.v1.summary.image('groundtruth', groundtruths, 2)\n",
    "            tf.compat.v1.summary.image('reconstruction', reconstructions, 2)\n",
    "\n",
    "            merged = tf.compat.v1.summary.merge_all()\n",
    "            writer = tf.compat.v1.summary.FileWriter(tensorboard_path, sess.graph)\n",
    "\n",
    "        print('Done.')\n",
    "\n",
    "\n",
    "        iters = 0\n",
    "        saver = tf.compat.v1.train.Saver(max_to_keep=5)\n",
    "        if args.checkpoint_path is None:\n",
    "            sess.run(tf.compat.v1.global_variables_initializer())\n",
    "        else:\n",
    "            print('Start loading checkpoint...')\n",
    "            saver.restore(sess, args.checkpoint_path)\n",
    "            iters = args.resume_step\n",
    "            print('Done.')\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        print('Start training...')\n",
    "        \n",
    "        for epoch in range(args.epoch):\n",
    "            \n",
    "            if epoch > args.lr_decay_epoch:\n",
    "                learning_rate_val = args.base_lr / 10\n",
    "            else:\n",
    "                learning_rate_val = args.base_lr\n",
    "                       \n",
    "            for start, end in zip(\n",
    "                    range(0, args.trainset_length, args.batch_size),\n",
    "                    range(args.batch_size, args.trainset_length, args.batch_size)):\n",
    "\n",
    "                if iters == 0 and args.checkpoint_path is None:\n",
    "                    print('Start pretraining G!')\n",
    "                    for t in range(args.warmup_steps):\n",
    "                        if t % 20 == 0:\n",
    "                            print(\"Step:\", t)\n",
    "                        images = sess.run([train_im])[0]\n",
    "                        if len(images) < args.batch_size:\n",
    "                            images = sess.run([train_im])[0]\n",
    "\n",
    "                        inp_dict = {}\n",
    "                        inp_dict = loss.feed_all_gpu(inp_dict, args.num_gpu, args.batch_size_per_gpu, images, params)\n",
    "                        inp_dict[learning_rate] = learning_rate_val\n",
    "                        inp_dict[lambda_rec] = 1.\n",
    "\n",
    "                        _ = sess.run(\n",
    "                            [train_op_G],\n",
    "                            feed_dict=inp_dict)\n",
    "                    print('Pre-train G Done!')\n",
    "\n",
    "                if (iters < 25 and args.checkpoint_path is None) or iters % 500 == 0:\n",
    "                    n_cir = 30\n",
    "                else:\n",
    "                    n_cir = args.critic_steps\n",
    "\n",
    "                for t in range(n_cir):\n",
    "                    images = sess.run([train_im])[0]\n",
    "                    if len(images) < args.batch_size:\n",
    "                        images = sess.run([train_im])[0]\n",
    "\n",
    "                    inp_dict = {}\n",
    "                    inp_dict = loss.feed_all_gpu(inp_dict, args.num_gpu, args.batch_size_per_gpu, images, params)\n",
    "                    inp_dict[learning_rate] = learning_rate_val\n",
    "                    inp_dict[lambda_rec] = args.lambda_rec\n",
    "\n",
    "                    _ = sess.run(\n",
    "                        [train_op_D],\n",
    "                        feed_dict=inp_dict)\n",
    "\n",
    "\n",
    "                if iters % 50 == 0:\n",
    "\n",
    "                    _, g_val, ag_val, rs, d_val = sess.run(\n",
    "                        [train_op_G, aver_loss_g, aver_loss_ag, merged, aver_loss_d],\n",
    "                        feed_dict=inp_dict)\n",
    "                    writer.add_summary(rs, iters)\n",
    "\n",
    "                else:\n",
    "\n",
    "                    _, g_val, ag_val, d_val = sess.run(\n",
    "                        [train_op_G, aver_loss_g, aver_loss_ag, aver_loss_d],\n",
    "                        feed_dict=inp_dict)\n",
    "                if iters % 20 == 0:\n",
    "                    print(\"Iter:\", iters, 'loss_g:', g_val, 'loss_d:', d_val, 'loss_adv_g:', ag_val)\n",
    "\n",
    "                iters += 1\n",
    "\n",
    "            saver.save(sess, model_path, global_step=iters)\n",
    "\n",
    "            # testing\n",
    "            if epoch > 0:\n",
    "                ii = 0\n",
    "                g_vals = 0\n",
    "                d_vals = 0\n",
    "                ag_vals = 0\n",
    "                n_batchs = 0\n",
    "                for _ in range(int(args.testset_length / args.batch_size)):\n",
    "                    test_oris = sess.run([test_im])[0]\n",
    "                    if len(test_oris) < args.batch_size:\n",
    "                        test_oris = sess.run([test_im])[0]\n",
    "\n",
    "                    inp_dict = {}\n",
    "                    inp_dict = loss.feed_all_gpu(inp_dict, args.num_gpu, args.batch_size_per_gpu, test_oris, params)\n",
    "                    inp_dict[learning_rate] = learning_rate_val\n",
    "                    inp_dict[lambda_rec] = args.lambda_rec\n",
    "\n",
    "                    reconstruction_vals, g_val, d_val, ag_val = sess.run(\n",
    "                        [reconstruction, aver_loss_g, aver_loss_d, aver_loss_ag],\n",
    "                        feed_dict=inp_dict)\n",
    "\n",
    "                    g_vals += g_val\n",
    "                    d_vals += d_val\n",
    "                    ag_vals += ag_val\n",
    "                    n_batchs += 1\n",
    "\n",
    "                    # Save test result every 100 epochs\n",
    "                    if epoch % 100 == 0:\n",
    "\n",
    "                        for rec_val, test_ori in zip(reconstruction_vals, test_oris):\n",
    "                            rec_hid = (255. * (rec_val + 1) /\n",
    "                                       2.).astype(np.uint8)\n",
    "                            test_ori = (255. * (test_ori + 1) /\n",
    "                                        2.).astype(np.uint8)\n",
    "                            Image.fromarray(rec_hid).save(os.path.join(\n",
    "                                result_path, 'img_' + str(ii) + '.' + str(int(iters / 100)) + '.jpg'))\n",
    "                            if epoch == 0:\n",
    "                                Image.fromarray(test_ori).save(\n",
    "                                    os.path.join(result_path, 'img_' + str(ii) + '.' + str(int(iters / 100)) + '.ori.jpg'))\n",
    "                            ii += 1\n",
    "                g_vals /= n_batchs\n",
    "                d_vals /= n_batchs\n",
    "                ag_vals /= n_batchs\n",
    "\n",
    "                summary = tf.Summary()\n",
    "                summary.value.add(tag='eval/g',\n",
    "                                  simple_value=g_vals)\n",
    "                summary.value.add(tag='eval/d',\n",
    "                                  simple_value=d_vals)\n",
    "                summary.value.add(tag='eval/ag',\n",
    "                                  simple_value=ag_vals)\n",
    "                writer.add_summary(summary, iters)\n",
    "\n",
    "                print(\"=========================================================================\")\n",
    "                print('loss_g:', g_val, 'loss_d:', d_val, 'loss_adv_g:', ag_val)\n",
    "                print(\"=========================================================================\")\n",
    "\n",
    "                if np.isnan(reconstruction_vals.min()) or np.isnan(reconstruction_vals.max()):\n",
    "                    print(\"NaN detected!!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
