{"cells":[{"cell_type":"code","execution_count":null,"id":"rDbxfSM8vGRP","metadata":{"executionInfo":{"elapsed":152,"status":"ok","timestamp":1703266389315,"user":{"displayName":"Chris Sibbitt","userId":"00854878476520643924"},"user_tz":300},"id":"rDbxfSM8vGRP"},"outputs":[],"source":["! if [ ! -f tf_scenery.zip ]; then wget \"https://drive.google.com/uc?export=download&id=1XcL0guFyqhLns_HgkFBEKEpbUHQ1dA6U&confirm=yes\" -O tf_scenery.zip; unzip tf_scenery.zip; fi"]},{"cell_type":"code","execution_count":null,"id":"D0qWNIhCtN2y","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":899,"status":"ok","timestamp":1703266390204,"user":{"displayName":"Chris Sibbitt","userId":"00854878476520643924"},"user_tz":300},"id":"D0qWNIhCtN2y","outputId":"8ac3abac-f982-4193-cdc4-e2d4c25c90f8"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('./drive')"]},{"cell_type":"code","execution_count":null,"id":"6e0ecc01-776d-423f-b305-90df9e358b0c","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29602,"status":"ok","timestamp":1703266419802,"user":{"displayName":"Chris Sibbitt","userId":"00854878476520643924"},"user_tz":300},"id":"6e0ecc01-776d-423f-b305-90df9e358b0c","outputId":"e246300f-1074-4f09-9dff-caed5dbf68e6"},"outputs":[],"source":["!pip install tensorflow-addons\n","import random\n","import os\n","import sys\n","sys.path.append('./drive/MyDrive/v2_NS-Outpainting')\n","from glob import glob\n","import numpy as np\n","from PIL import Image\n","import tensorflow as tf\n","from tensorflow.python.training.moving_averages import assign_moving_average\n","from modeling.model import Model\n","from modeling.loss import Loss\n","from dataset.parse import parse_trainset, parse_testset\n","import argparse"]},{"cell_type":"code","execution_count":null,"id":"db68f1be-56b2-485c-a5af-3fc0d8065299","metadata":{"executionInfo":{"elapsed":39,"status":"ok","timestamp":1703266419803,"user":{"displayName":"Chris Sibbitt","userId":"00854878476520643924"},"user_tz":300},"id":"db68f1be-56b2-485c-a5af-3fc0d8065299"},"outputs":[],"source":["parser = argparse.ArgumentParser(description='Model training.')\n","# experiment\n","parser.add_argument('--date', type=str, default='1221')\n","parser.add_argument('--exp-index', type=int, default=1)\n","parser.add_argument('--f', action='store_true', default=False)\n","\n","# gpu\n","parser.add_argument('--start-gpu', type=int, default=0)\n","parser.add_argument('--num-gpu', type=int, default=2)\n","\n","# dataset\n","parser.add_argument('--trainset-path', type=str, default='./dataset/trainset.tfr')\n","parser.add_argument('--testset-path', type=str, default='./dataset/testset.tfr')\n","parser.add_argument('--trainset-length', type=int, default=5041)\n","parser.add_argument('--testset-length', type=int, default=2000)  # we flip every image in testset\n","\n","# training\n","parser.add_argument('--base-lr', type=float, default=0.0001)\n","parser.add_argument('--batch-size', type=int, default=32)\n","parser.add_argument('--weight-decay', type=float, default=0.00002)\n","parser.add_argument('--epoch', type=int, default=1500)\n","parser.add_argument('--lr-decay-epoch', type=int, default=1000)\n","parser.add_argument('--critic-steps', type=int, default=3)\n","parser.add_argument('--warmup-steps', type=int, default=1000)\n","parser.add_argument('--workers', type=int, default=2)\n","parser.add_argument('--clip-gradient', action='store_true', default=False)\n","parser.add_argument('--clip-gradient-value', type=float, default=0.1)\n","\n","\n","# modeling\n","parser.add_argument('--beta', type=float, default=0.9)\n","parser.add_argument('--lambda-gp', type=float, default=10)\n","parser.add_argument('--lambda-rec', type=float, default=0.998)\n","\n","# checkpoint\n","parser.add_argument('--log-path', type=str, default='./logs/')\n","parser.add_argument('--checkpoint-path', type=str, default=None)\n","parser.add_argument('--resume-step', type=int, default=0)\n","\n","args = parser.parse_args(['--f', '--trainset-path', './tf_dataset_new/trainset.tfr', '--testset-path', './tf_dataset_new/testset.tfr', '--log-path', './drive/MyDrive/v2_NS-Outpainting/logs/', '--batch-size', '4', '--num-gpu', '8']) #, '--checkpoint-path', './drive/MyDrive/v2_NS-Outpainting/logs/1221/1/models/-24963', '--resume-step', '24963'])"]},{"cell_type":"markdown","id":"eJ2n-bspit2z","metadata":{"id":"eJ2n-bspit2z"},"source":[]},{"cell_type":"code","execution_count":null,"id":"9e7bf839-96fb-4050-87c6-52b0e34b54c3","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":152,"status":"ok","timestamp":1703266419918,"user":{"displayName":"Chris Sibbitt","userId":"00854878476520643924"},"user_tz":300},"id":"9e7bf839-96fb-4050-87c6-52b0e34b54c3","outputId":"e113d1e6-c9e6-4c86-f0fe-c0eda766eadb"},"outputs":[],"source":["# prepare path\n","base_path = args.log_path\n","exp_date = args.date\n","if exp_date is None:\n","    print('Exp date error!')\n","    import sys\n","    sys.exit()\n","exp_name = exp_date + '/' + str(args.exp_index)\n","print(\"Start Exp:\", exp_name)\n","output_path = base_path + exp_name + '/'\n","model_path = output_path + 'models/'\n","tensorboard_path = output_path + 'log/'\n","result_path = output_path + 'results/'\n","\n","if not os.path.exists(model_path):\n","    os.makedirs(model_path)\n","if not os.path.exists(tensorboard_path):\n","    os.makedirs(tensorboard_path)\n","if not os.path.exists(result_path):\n","    os.makedirs(result_path)\n","elif not args.f:\n","    if args.checkpoint_path is None:\n","        print('Exp exist!')\n","        import sys\n","        sys.exit()\n","else:\n","    import shutil\n","    shutil.rmtree(model_path)\n","    os.makedirs(model_path)\n","    shutil.rmtree(tensorboard_path)\n","    os.makedirs(tensorboard_path)"]},{"cell_type":"code","execution_count":null,"id":"c761d337-c924-4396-b3e8-fca323054d00","metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1703266419919,"user":{"displayName":"Chris Sibbitt","userId":"00854878476520643924"},"user_tz":300},"id":"c761d337-c924-4396-b3e8-fca323054d00"},"outputs":[],"source":["# prepare gpu\n","num_gpu = args.num_gpu\n","start_gpu = args.start_gpu\n","gpu_id = str(start_gpu)\n","for i in range(num_gpu - 1):\n","    gpu_id = gpu_id + ',' + str(start_gpu + i + 1)\n","os.environ['CUDA_VISIBLE_DEVICES'] = str(gpu_id)\n","args.batch_size_per_gpu = int(args.batch_size / args.num_gpu)"]},{"cell_type":"code","execution_count":null,"id":"f12515d6-32c6-4611-8da1-3e8922f8b2f3","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":757},"executionInfo":{"elapsed":28535,"status":"error","timestamp":1703266448450,"user":{"displayName":"Chris Sibbitt","userId":"00854878476520643924"},"user_tz":300},"id":"f12515d6-32c6-4611-8da1-3e8922f8b2f3","outputId":"c0b85138-c2a9-40c3-f983-b4c4ac8a660e"},"outputs":[],"source":["# #**\n","# resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')\n","# tf.config.experimental_connect_to_cluster(resolver)\n","# tf.tpu.experimental.initialize_tpu_system(resolver)\n","# print(\"All devices: \", tf.config.list_logical_devices('TPU'))\n","# #**\n","model = Model(args)\n","loss = Loss(args)\n","\n","config = tf.compat.v1.ConfigProto(allow_soft_placement=True)\n","config.gpu_options.allow_growth = True\n","config.graph_options.optimizer_options.global_jit_level = tf.compat.v1.OptimizerOptions.ON_1\n","\n","print(\"Start building model...\")\n","with tf.compat.v1.Session(config=config) as sess:\n","    with tf.device('/cpu:0'):\n","        learning_rate = tf.compat.v1.placeholder(tf.float32, [])\n","        lambda_rec = tf.compat.v1.placeholder(tf.float32, [])\n","\n","        train_op_G = tf.compat.v1.train.AdamOptimizer(\n","            learning_rate=learning_rate, beta1=0.5, beta2=0.9)\n","        train_op_D = tf.compat.v1.train.AdamOptimizer(\n","            learning_rate=learning_rate, beta1=0.5, beta2=0.9)\n","\n","\n","        trainset = tf.compat.v1.data.TFRecordDataset(filenames=[args.trainset_path])\n","        trainset = trainset.shuffle(args.trainset_length)\n","        trainset = trainset.map(parse_trainset, num_parallel_calls=args.workers)\n","        trainset = trainset.batch(args.batch_size).repeat()\n","\n","        train_iterator = trainset.make_one_shot_iterator()\n","        train_im = train_iterator.get_next()\n","\n","        testset = tf.compat.v1.data.TFRecordDataset(filenames=[args.testset_path])\n","        testset = testset.map(parse_testset, num_parallel_calls=args.workers)\n","        testset = testset.batch(args.batch_size).repeat()\n","\n","        test_iterator = testset.make_one_shot_iterator()\n","        test_im = test_iterator.get_next()\n","\n","        print('build model on gpu tower')\n","        models = []\n","        params = []\n","        for gpu_id in range(num_gpu):\n","          with tf.device('/tpu:%d' % gpu_id): #**\n","            print('tower_%d' % gpu_id)\n","            with tf.name_scope('tower_%d' % gpu_id):\n","                with tf.compat.v1.variable_scope('cpu_variables', reuse=gpu_id > 0):\n","\n","                    groundtruth = tf.compat.v1.placeholder(\n","                        tf.float32, [args.batch_size_per_gpu, 128, 256, 3], name='groundtruth')\n","                    left_gt = tf.slice(groundtruth, [0, 0, 0, 0], [args.batch_size_per_gpu, 128, 128, 3])\n","\n","\n","                    reconstruction_ori, reconstruction = model(left_gt)\n","                    right_recon = tf.slice(reconstruction, [0, 0, 128, 0], [args.batch_size_per_gpu, 128, 128, 3])\n","\n","                    loss_rec = loss.masked_reconstruction_loss(groundtruth, reconstruction)\n","                    loss_adv_G, loss_adv_D = loss.global_and_local_adv_loss(model, groundtruth, reconstruction)\n","\n","                    reg_losses = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.REGULARIZATION_LOSSES)\n","                    loss_G = loss_adv_G * (1 - lambda_rec) + loss_rec * lambda_rec + sum(reg_losses)\n","                    loss_D = loss_adv_D\n","\n","                    var_G = list(filter(lambda x: x.name.startswith(\n","                        'cpu_variables/GEN'), tf.compat.v1.trainable_variables()))\n","                    var_D = list(filter(lambda x: x.name.startswith(\n","                        'cpu_variables/DIS'), tf.compat.v1.trainable_variables()))\n","\n","\n","                    grad_g = train_op_G.compute_gradients(\n","                        loss_G, var_list=var_G)\n","                    grad_d = train_op_D.compute_gradients(\n","                        loss_D, var_list=var_D)\n","\n","                    models.append((grad_g, grad_d, loss_G, loss_D, loss_adv_G, loss_rec, reconstruction))\n","                    params.append(groundtruth)\n","\n","    print('Done.')\n","\n","    print('Start reducing towers on gpu...')\n","\n","    grad_gs, grad_ds, loss_Gs, loss_Ds, loss_adv_Gs, loss_recs, reconstructions = zip(*models)\n","    groundtruths = params\n","\n","    with tf.device('/tpu:0'): #** gpu\n","        aver_loss_g = tf.reduce_mean(input_tensor=loss_Gs)\n","        aver_loss_d = tf.reduce_mean(input_tensor=loss_Ds)\n","        aver_loss_ag = tf.reduce_mean(input_tensor=loss_adv_Gs)\n","        aver_loss_rec = tf.reduce_mean(input_tensor=loss_recs)\n","\n","        train_op_G = train_op_G.apply_gradients(\n","            loss.average_gradients(grad_gs))\n","        train_op_D = train_op_D.apply_gradients(\n","            loss.average_gradients(grad_ds))\n","\n","        groundtruths = tf.concat(groundtruths, axis=0)\n","        reconstructions = tf.concat(reconstructions, axis=0)\n","\n","        tf.compat.v1.summary.scalar('loss_g', aver_loss_g)\n","        tf.compat.v1.summary.scalar('loss_d', aver_loss_d)\n","        tf.compat.v1.summary.scalar('loss_ag', aver_loss_ag)\n","        tf.compat.v1.summary.scalar('loss_rec', aver_loss_rec)\n","        tf.compat.v1.summary.image('groundtruth', groundtruths, 2)\n","        tf.compat.v1.summary.image('reconstruction', reconstructions, 2)\n","\n","        merged = tf.compat.v1.summary.merge_all()\n","        writer = tf.compat.v1.summary.FileWriter(tensorboard_path, sess.graph)\n","\n","    print('Done.')\n","\n","    iters = 0\n","    saver = tf.compat.v1.train.Saver(max_to_keep=2)\n","    if args.checkpoint_path is None:\n","        sess.run(tf.compat.v1.global_variables_initializer())\n","    else:\n","        print('Start loading checkpoint...')\n","        saver.restore(sess, args.checkpoint_path)\n","        iters = args.resume_step\n","        print('Done.')\n","\n","    print('Start training...')\n","\n","    for epoch in range(args.epoch):\n","\n","        if epoch > args.lr_decay_epoch:\n","            learning_rate_val = args.base_lr / 10\n","        else:\n","            learning_rate_val = args.base_lr\n","\n","        for start, end in zip(\n","                range(0, args.trainset_length, args.batch_size),\n","                range(args.batch_size, args.trainset_length, args.batch_size)):\n","\n","            if iters == 0 and args.checkpoint_path is None:\n","                print('Start pretraining G!')\n","                for t in range(args.warmup_steps):\n","                    if t % 20 == 0:\n","                        print(\"Step:\", t)\n","                    images = sess.run([train_im])[0]\n","                    if len(images) < args.batch_size:\n","                        images = sess.run([train_im])[0]\n","\n","                    inp_dict = {}\n","                    inp_dict = loss.feed_all_gpu(inp_dict, args.num_gpu, args.batch_size_per_gpu, images, params)\n","                    inp_dict[learning_rate] = learning_rate_val\n","                    inp_dict[lambda_rec] = 1.\n","\n","                    _ = sess.run(\n","                        [train_op_G],\n","                        feed_dict=inp_dict)\n","                print('Pre-train G Done!')\n","\n","            if (iters < 25 and args.checkpoint_path is None) or iters % 500 == 0:\n","                n_cir = 30\n","            else:\n","                n_cir = args.critic_steps\n","\n","            for t in range(n_cir):\n","                images = sess.run([train_im])[0]\n","                if len(images) < args.batch_size:\n","                    images = sess.run([train_im])[0]\n","\n","                inp_dict = {}\n","                inp_dict = loss.feed_all_gpu(inp_dict, args.num_gpu, args.batch_size_per_gpu, images, params)\n","                inp_dict[learning_rate] = learning_rate_val\n","                inp_dict[lambda_rec] = args.lambda_rec\n","\n","                _ = sess.run(\n","                    [train_op_D],\n","                    feed_dict=inp_dict)\n","\n","\n","            if iters % 50 == 0:\n","\n","                _, g_val, ag_val, rs, d_val = sess.run(\n","                    [train_op_G, aver_loss_g, aver_loss_ag, merged, aver_loss_d],\n","                    feed_dict=inp_dict)\n","                writer.add_summary(rs, iters)\n","\n","            else:\n","\n","                _, g_val, ag_val, d_val = sess.run(\n","                    [train_op_G, aver_loss_g, aver_loss_ag, aver_loss_d],\n","                    feed_dict=inp_dict)\n","            if iters % 20 == 0:\n","                print(\"Iter:\", iters, 'loss_g:', g_val, 'loss_d:', d_val, 'loss_adv_g:', ag_val)\n","\n","            iters += 1\n","\n","        saver.save(sess, model_path, global_step=iters)\n","\n","        # testing\n","        if epoch > 0:\n","            ii = 0\n","            g_vals = 0\n","            d_vals = 0\n","            ag_vals = 0\n","            n_batchs = 0\n","            for _ in range(int(args.testset_length / args.batch_size)):\n","                test_oris = sess.run([test_im])[0]\n","                if len(test_oris) < args.batch_size:\n","                    test_oris = sess.run([test_im])[0]\n","\n","                inp_dict = {}\n","                inp_dict = loss.feed_all_gpu(inp_dict, args.num_gpu, args.batch_size_per_gpu, test_oris, params)\n","                inp_dict[learning_rate] = learning_rate_val\n","                inp_dict[lambda_rec] = args.lambda_rec\n","\n","                reconstruction_vals, g_val, d_val, ag_val = sess.run(\n","                    [reconstruction, aver_loss_g, aver_loss_d, aver_loss_ag],\n","                    feed_dict=inp_dict)\n","\n","                g_vals += g_val\n","                d_vals += d_val\n","                ag_vals += ag_val\n","                n_batchs += 1\n","\n","                # Save test result every 100 epochs\n","                if epoch % 100 == 0:\n","\n","                    for rec_val, test_ori in zip(reconstruction_vals, test_oris):\n","                        rec_hid = (255. * (rec_val + 1) /\n","                                  2.).astype(np.uint8)\n","                        test_ori = (255. * (test_ori + 1) /\n","                                    2.).astype(np.uint8)\n","                        Image.fromarray(rec_hid).save(os.path.join(\n","                            result_path, 'img_' + str(ii) + '.' + str(int(iters / 100)) + '.jpg'))\n","                        if epoch == 0:\n","                            Image.fromarray(test_ori).save(\n","                                os.path.join(result_path, 'img_' + str(ii) + '.' + str(int(iters / 100)) + '.ori.jpg'))\n","                        ii += 1\n","            g_vals /= n_batchs\n","            d_vals /= n_batchs\n","            ag_vals /= n_batchs\n","\n","            summary = tf.compat.v1.Summary()\n","            summary.value.add(tag='eval/g',\n","                              simple_value=g_vals)\n","            summary.value.add(tag='eval/d',\n","                              simple_value=d_vals)\n","            summary.value.add(tag='eval/ag',\n","                              simple_value=ag_vals)\n","            writer.add_summary(summary, iters)\n","\n","            print(\"=========================================================================\")\n","            print('loss_g:', g_val, 'loss_d:', d_val, 'loss_adv_g:', ag_val)\n","            print(\"=========================================================================\")\n","\n","            if np.isnan(reconstruction_vals.min()) or np.isnan(reconstruction_vals.max()):\n","                print(\"NaN detected!!\")"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"https://github.com/csibbitt/NS-Outpainting/blob/master/train_model.ipynb","timestamp":1702613933530}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":5}
