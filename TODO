Model
---
* Faster startup for testing (limit dataset load)
* Refactor layout of models vs layers vs utilities
* Refactor to tf.Loss (do I need multiples?)
* Utility to rename files with their hash
* Figure out this NaN problem
  * Can I load a broken checkpoint and find the source?
* Full TF2 validation training
* Huggingface compatibility
* Profiling
* High rez training
* Stability

GUI
---
* Soft-start
  * Scale the fps up/dn subtley to adjust the position without skipping the initial frames
* Instant mix-in
* gallery of seed images with mixin controls
* Some sort of bugs with buffer size getting out of sync and stuff
* Split into GUI w/ model in separate python module repo


On T4@Colab:
Epocs: 1500 @250s = 378,000s (105hrs)
Iters: 235500 @1.3s = 316,800s (88hrs)

On 3090@Genesis:
Epocs: 1500 @64s = 96,000s (26hrs)
Iters: 235500 @ 0.2s = 47,100 (13hrs)


 # --load-v2-checkpoint --checkpoint-path logs/20240108/920/checkpoint/ckpt-25",


WARNING:tensorflow:5 out of the last 5 calls to <function _BaseOptimizer._update_step_xla at 0x7f964062ea70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
