Model
---
* Test the v2 checkpoint from overnight
  * Load a middle one and record loss values
  * Load an earlier one and run it to the same iter to check loss values (epoch bug!)
* Merge eval/run into one thing with cmdline vs non-cmdline
* Eliminate session usage in run_model
* Faster startup for testing (limit dataset load)
* Eliminate @tf.compat.v1.keras.utils.track_tf1_style_variables
* Eliminate get_or_create_layer
* Eliminate session usage in train_model
* Replace leaky-relu with tf
* Refactor layout of models vs layers vs utilities
* Upgrade best training checkpoint to V2 checkpoint
* Refactor to tf.Loss (do I need multiples?)
* Utility to rename files with their hash
* Figure out this NaN problem
  * Can I load a broken checkpoint and find the source?
* Full TF2 validation training
* Huggingface compatibility
* Profiling
* High rez training
* Stability

GUI
---
* Soft-start
  * Scale the fps up/dn subtley to adjust the position without skipping the initial frames
* Instant mix-in
* gallery of seed images with mixin controls
* Some sort of bugs with buffer size getting out of sync and stuff
* Split into GUI w/ model in separate python module repo


On T4@Colab:
Epocs: 1500 @250s = 378,000s (105hrs)
Iters: 235500 @1.3s = 316,800s (88hrs)

On 3090@Genesis:
Epocs: 1500 @64s = 96,000s (26hrs)
Iters: 235500 @ 0.2s = 47,100 (13hrs)


 # --load-v2-checkpoint --checkpoint-path logs/20240108/920/checkpoint/ckpt-25",



            lstm_cell = tf.compat.v1.nn.rnn_cell.LSTMCell(
                4 * self.size, activation=activation_fn)
            lstm_cell = tf.compat.v1.nn.rnn_cell.MultiRNNCell( # WARNING:tensorflow:At least two cells provided to MultiRNNCell are the same object and will share weights.
                [lstm_cell] * layer_num, state_is_tuple=True)
